%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
	20 20 moveto
	20 220 lineto
	220 220 lineto
	220 20 lineto
closepath
2 setlinewidth
gsave
	.4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{algorithmic}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumerate}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{geometry}
\usepackage{float}
\usepackage{indentfirst}
\usepackage[colorlinks,
linkcolor=blue,
anchorcolor=black,
citecolor=blue]{hyperref}
\geometry{top=2.5cm,bottom=2.5cm}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}
	
\title{Vital Information Matching in Vision-and-language Navigation %\thanks{Grants or other notes
		%about the article that should go on the front page should be
		%placed here. General acknowledgments should be placed at the end of the article.}
}
	%\subtitle{Do you have a subtitle?\\ If so, write it here}
	
	%\titlerunning{Short form of title}        % if too long for running head
	
	\author{xx* $^{1}$ \and xx $^{1}$ \and xx*$^{1}$ \and  xx $^{1}$ \and  xx $^{1}$ \and xx $^{1}$ \and xx $^{1}$}
	
	%\authorrunning{Short form of author list} % if too long for running head
	
	\institute{xx* \at
		\email{xx}   
		\\
		\and
		xx \at
		\email{xx \\} 
		\and
		xx* \at
		\email{xx \\} 
		%             \emph{Present address:} of F. Author  %  if needed
		\\
		$^{1}$  Faculty of Robot Science and Engineering, Northeastern University, Shenyang 110819, China 
	}
	
	\date{Received: date / Accepted: date}
	% The correct dates will be entered by the editor
	
	
\maketitle
\begin{abstract}
		
With the rapid growth of computer vision and natural language processing technologies, more and more researchers have paid attention on the Vision-and-Language navigation which is one of the most important tasks in multimodal machine learning. Undoubtedly, the focus of the multimodal field is how to fuse multiple inputs, which is crucial to integrate the intrinsic information. However, the existing models are only realized through simple data enhancement or expansion, which is obviously far from being able to dig out the inner connections between modalities. In this paper, in order to overcome these challenges, we propose a new multimodal matching model which is a brand new neural network called Vital Information Matching Networks (VIM-Net). Our VIM-Net consists of two matching modules, the vision matching module (V-mat) and the trajectory matching module (T-mat). Specifically, V-mat is to match the target information identified from vision with the entity information extracted from instructions; T-mat is to match the serialized trajectory feature with the movement direction from instructions. Benefiting from the collaborative learning of V-mat and T-mat, our VIM-Net has the ability to unearth close connections between different modalities. The performance of the proposed model on various operations is then experimentally demonstrated and compared with other models using the Matterport3D Simulator and room-to-room (R2R) benchmark dataset. 
		\keywords{Vision-and-language Navigation \and Multimodal Matching \and Self-tuning Module  \and Vital Information Matching Networks}
		% \PACS{PACS code1 \and PACS code2 \and more}
		% \subclass{MSC code1 \and MSC code2 \and more}
		
\end{abstract}
	
\section{Introduction}
\label{intro}

Scholars have proposed that the VLN task hopes that the robot can have the ability to ask, answer and do. In essence, it is hoped that the machine can understand and process visual information, language information, and output corresponding action information to complete a higher degree of cross-domain information fusion. This task needs to understand the natural language instructions and the image information that can be seen in the perspective at the same time. Then the agent can ensure its position in the environment and take the corresponding action to reach the destination. In recent years, Anderson \cite{anderson2018vision} first proposed a relatively simple sequence to sequence (Seq2Seq) learning with neural network in which an action sequence is output from two inputs sequences. Then, various data augmentation techniques were used to optimize the effect of R2R dataset training model, including Fried \cite{fried2018speaker}, Zhu \cite{zhu20201vision}, Jain \cite{jain2019stay}. Therefore, to promote the solution of this problem, focusing on improving the ability of perceptual vision and language input multimodal matching has become a research hotspot in this field.

The above introduces that many scholars have made their own contributions to solving the fusion of different modal\cite{fried2018speaker,landi2019embodied,hwang2021joint}, but this direction has not been solved well. In my opinion, there are two main reasons for improving the multimodality of perceptual information is challenging. First of all, the vital features cannot be accurately predicted of visual and textual information, including landmark objects in the scene or landmark direction in the instructions. Secondly, the inner connections between perceptual inputs may only be reflected in a little vital information for complex indoor scenes and if these connections are ignored, this will increase challenges of the task. In order to overcome these challenges, we propose a new multimodal matching model which is a brand new neural network called Vital Information Matching Networks (VIM-Net).

Not only for the VLN field, improving the perception of input multimodal matching is also very helpful for perfecting multimodal intelligence tasks in other fields. For example, visual question answering (VQA) \cite{antol2015vqa}, which generates answers to natural language questions on the presented images; image/video subtitle generation, generates description text on the premise of understanding the inputs image or video content \cite{das2017visual}, etc. If there is no way to properly fuse the input features of the network and directly splice all the inputs and input them into the entire model, a lot of useful information will be ignored. Therefore, improving the multimodal matching ability is of great significance to the optimization of the entire multimodal intelligence task.

In general, we propose a new multi-modal matching method and navigation self-tuning module, which is a new neural network model called VIM-Net. Our VIM-Net consists of two matching modules, the vision matching module (V-mat) and the trajectory matching module (T-mat). Specifically, V-mat is to match the target information identified from the vision with the entity information extracted from the instructions. T-mat is to match movements identified from the trajectory with the direction information extracted from the instructions. In the original vision-and-language information, it focuses on the overall feature fusion between the modalities, thereby improving the overall multimodal matching ability is crucial. In view of the fact that the traditional processing method only improves the matching effect by increasing the data quantity, it greatly ignores the characteristics of the dataset itself. Therefore, we follow the way how humans to know their position and take the next step in the process of path finding, which will give priority to matching whether to arrive at the key position, so as to determine whether to complete the navigation at this stage and proceed to the next stage of trajectory planning. More than that, we also sample the path generated by the action predictor as the third type of data information to match the text instructions. In addition, T-mat is aimed at comparing the difference between the target trajectory and the actual path. Then, if the path after the matching score is lower than the threshold, it should be corrected and return to the previous location. The heuristic self-tuning module is applied to retrain and adjust the parameters to plan a more reliable path, thereby improving the accuracy of reaching the designated destination. In general, our contributions include:

\begin{itemize}
		\item[](1) A new neural network model VIM-Net is developed, which uses the information after image target recognition to match the extracted features of language entities to optimize the utilization of dataset.
		\item[](2) The Self-tuning module based on heuristics makes the generated path match the language instructions, and retrains the parameters at the appropriate position to achieve the effect of optimizing the path and improving the robustness of the entire networks.
		\item[](3) We have compared the result with baseline on the R2R dataset, and the experimental results show that the VIM-Net can greatly improve the navigation accuracy of VLN tasks.
\end{itemize}
\section{Motivation}
	
In this section, we firstly explain that it is universal to ignore the object matching of vision and language features. Then, we introduce our preliminary idea. Finally, in response to the above ideas, the measures to be taken are proposed.
\vspace{-1em}
\subsection{Current Situation}	

\begin{figure*}[h]

	\centering
	\includegraphics[scale=1]{image01.png}
	\caption{Examples of vision and language navigation (VLN) tasks are executed by VIM agent.}
	\label{image01}
\end{figure*}

As Figure \ref{image01} shows, the perceptual vision contains a large amount of environmental information \cite{huang2019transferable}, such as various tables, decorations, room layout, etc. Changeable natural language instructions require agents to perform compound actions \cite{majumdar2020improving}, such as `turn right after passing the door, the dining room is on your right` and `go straight and pass the dining room and you will arrive at the living room`. Therefore, in order to reach destination accurately, the VLN agent is of great significance to details of the two modalities. In particular, three steps should be performed. Step 1: it needs to locate the agent and determine the next action; step 2: it needs to identify landmarks from the perceptual vision; and step 3: it needs to choose the single action to implements the compound action in accordance with instructions.

What's more, the agent should be able to make corresponding actions according to multimodal feature which is consistent data fussion based on the natural language instructions and real-time vision \cite{qi2020object}. However, the previous work mainly focused on increasing amount of input data and ignoring the matching between the target information and vital information of instructions. Whereas the large proportion of useless characteristics, it is highly possible to generate unreliable paths \cite{ma2019regretful}. According to the dataset, each complete track is obtained by serializing a small track which has bad effect that the generated trajectory is more deviated from the original predetermined route, resulting in the poor navigation accuracy.
\vspace{-1em}
\subsection{Our Discovering}

In response to the above-mentioned situation, we found that the detailed matching information between perceptual vision, language instructions and serialized trajectories are often ignored. This phenomenon is not only in the R2R dataset, but also in other research fields, such as dialogue navigation, outdoor scene navigation, etc \cite{nguyen2019help,yan2019cross,chen2019touchdown}. However, this features can be extracted and fused in some cases. As shown in Figure \ref{image02}, we extracted keywords from instructions and the target features from vision which have a certain corresponding relationship. Specifically, it can be reflected in the appearance of sofa, TV, etc. under the first scene, which can correspond to the location of movie room. Since there is a right-turning action, we have no way to judge the right-turning angle well. we need to compare the confidence of the second and third perspectives, which is more suitbale for the dining room of instructions. If the correct direction is not selected at the beginning, we provide a self-tuning module so that the agent can return to the previous position to retrain parameters and then make a new choice. The rest of the conjunctions, etc., have no corresponding goals in the vision, and the most vital keywords occupy a relatively small proportion in the entire text, which will not play a leading role in the entire model \cite{hao2020towards}.
\begin{figure*}[h]
	\centering
	\includegraphics[scale=1]{image02.png}
	\caption{Examples of the keywords of instructions and the target features of vision  have certain corresponding relationship.}
	\label{image02}
	\vspace{-2em}
\end{figure*}

\subsection{Our Method Brief Introduction}

Based on the above dicovering, we summarized that fussion matching based on the target features of the perceptual vision and the keywords of the language instruction can improve the multimodal matching ability and optimize the limitation of single expanding the amount of dataset to improve accuracy. This method makes full use of the subtle connections between the two modalities, grasping the impact of vital information in the entire scene, and greatly optimizes the data input quality of the tasks. Not only that, we also explored the potential connection between the instructions and trajectories, and used the self-correction module to correct deviation in navigation. Considering this phenomenon, we proposed the VIM-Net network, a multimodal data fusion model based on vital information matching, to solve the problem that existing methods cannot make full use of internal characteristics of the dataset.

\section{Model Design}
	
In this section, we introduce the VIM-Net in detail, which matches the information after image target recognition with the features extracted from language entities. The modal contains two modules: vision-instruction matching module and trajectory instruction matching module. as the Figure \ref{image03} shows. In the vision-instruction matching module, we use the Yolo algorithm to extract the object features after target recognition and the features processed by the entity extraction component of instructions, compare them and input them into the action prediction module to guide the generation of navigation actions \cite{redmon2016you}.

\begin{figure*}[h]
	\centering
	\includegraphics[scale=1]{image03.png}
	\caption{Schema of the proposed architecture for VLN. The input instruction is vision, instruction and trajectory. The VIM-Net consists of Vital Information match module, Cross-model guide module and Self-tuning module.}
	\label{image03}
\end{figure*}

In order to further improve the overall integrity of the navigation, we introduced a self-correcting trajectory module that matches the path and the instruction. In this module, we refer to the feedback ideas of Ke \cite{ke2019tactical}, improve the relevant evaluation score indicators, and increase the robustness of the entire system.

\subsection{Vision and Instruction Matching Module}

In view of the fact that traditional processing methods only increase the matching effect by increasing the amount, only copy the data set, simply increase the number to improve the accuracy of the matching, or simply complicate the dataset (delete and modify the dataset or add other types of data), these methods still ignore the characteristics of the data set itself. Therefore, our visual instruction matching module is used to determine whether the agent has reached the landmark of the predetermined trajectory, which is the key matching part of the two types of information \cite{vasudevan2021talk2nav}. The image information obtained by the concrete vision is matched with the entity extracted from the language input. In the original visual and language information, the matching of entity information in the data stream is focused on, thereby improving the overall multimodal matching ability \cite{zhao2021evaluation}.

For panoramic images, the orientation features based on the pre-trained Reset-152 and the agent will be input into the visual features of the convolutional neural network (CNN). The institutional embedding dataset for each mode is input as the joint multimodal embedding module so that embedding features can be generated based on intermodal data exchange. Input the perceptual visual features processed by Yolo and the instruction keywords extracted by the entity extraction component to the visual instruction matching module, and set the controller of the scoring module as:

\begin{equation}
	\centering
	{\Psi}_{t}({s}_{t},{h}_{t-1}){\in}{({0},{1})}
\end{equation}

where ${1}$ indicates that the aimed landmark is reached and ${0}$ otherwise. ${{\Psi}_{t}}$ is an Adaptive Computation Time (ACT) LSTM which allows the controller to learn to make decisions at variable time steps. ${h}_{t-1}$ is the hidden state of the controller. In this work, ${{\Psi}_{t}}$ learns to identify the landmarks with the variable number of intermediate navigation steps.

\subsection{Track and Instruction Matching Module}

The inspiration for trajectory instruction matching comes from the fact that when people find their way, they will confirm whether they have not deviated from the trajectory when they reach a landmark or need to turn. Some scholars have made bold attempts before. Gordon and others used external memory MT to clearly remember the traversal path of the agent from the newly visited landmark\cite{vasudevan2021talk2nav}. When the agent reaches the iconic location, the memory MT is reinitialized to store the traversed path from the most recently visited landmark. This re-initialization can be understood as focusing on the most recently traversed path in order to better locate and better match the relevant direction indication through the trajectory instruction matching module.

\begin{figure*}[h]
	\centering
	\includegraphics[scale=1]{image06.png}
	\caption{Examples of navigation trajectory. The path is rasterized and written into the memory image.}
	\label{image06}
\end{figure*}

As the Figure \ref{image06} shows, since the agent is in the navigation process, we set up a write module to write the traversed path into the memory and calculate it from the traversal path in the simulation environment. Our writing module tracks the path from the most recently visited key point to the current location. The path is rasterized and written into the memory image. In the image, the path is represented by the red line, the starting point is marked by the blue square. The write module always writes from the center of the memory image to ensure that there is space in all directions. Whenever the coordinates of a new rasterized pixel exceeds the image size, the module incrementally increases the proportion of the stored image until the new pixel is in the image.

By recording the characteristics of the trajectory and inputting the corresponding instructions to the trajectory instruction matching module, the controller of the scoring module is set as:

\begin{equation}
	\centering
	{\varphi}_{t}({m}_{t},{h}_{t-1}){\in}{({0},{1})}
\end{equation}

where ${1}$ indicates that the aimed landmark is reached and ${0}$ otherwise. ${{\varphi}_{t}}$ is an Adaptive Computation Time (ACT) LSTM which allows the controller to learn to make decisions at variable time steps. ${h}_{t-1}$ is the hidden state of the controller. In this work, ${{\varphi}_{t}}$ learns to identify the landmarks with the variable number of intermediate navigation steps.

\subsection{Action prediction module}

The action at at time ${T}$ is defined as the weighted average of ${{\Psi}_{t}}$ and ${{\varphi}_{t}}$ . For different parts of the trajectory, the input of the action predictor mainly depends on  two inputs ${{\lambda}{\Psi}_{t} + {\varphi}_{t}}$ . For example, when the next Vital Information is not visible, the prediction should rely on ${{\varphi}_{t}}$ ; when the Vital Information is clearly identifiable, both inputs are input to the predictor. After the final data analysis, we determined that when the ${\lambda}$  is equal to 0.75, the overall network effect is the best. The learned matching score will adaptively decide which predictions are trustworthy and how many are passed in each step. This adaptive fusion can be understood as a calibration system of two complementary subsystems for motion prediction. The calibration method needs time or situation dependent. In this case, the input of the action predictor is enriched to the greatest extent, more accurate navigation actions are trained, and then the navigation trajectory with less error is serialized.

\subsection{Review of Target Detection Component}

\begin{figure*}[h]
	\centering
	\includegraphics[scale=1]{image04.png}
	\caption{Schema of the proposed architecture for Yolo.}
	\label{image04}
\end{figure*}
\vspace{-1em}
The target detection algorithm has made a great breakthrough. The more popular algorithms can be divided into two categories, one is the R-CNN algorithm based on region proposal (Fast R-CNN and Faster R-CNN). They are two-stage and need to use heuristic methods or CNN network to generate region proposals, and then perform classification and regression on the region proposals \cite{mao2019mini,girshick2015fast}. The other is one-stage algorithms such as Yolo and SSD , which only use a CNN network to directly predict the categories and positions of different targets. The first type of method is more accurate, but slower, but the second type of algorithm is faster, but the accuracy is lower.
	
as the Figure \ref{image04} shows, we use Yolo algorithm, which is detected by a CNN network. It is a single-pipe strategy, and its training and prediction are both end-to-end, so the Yolo algorithm is relatively simple and fast. Then, since Yolo convolves the entire picture, it has a larger field of view in the detection target, and it is not easy to misjudge the background.
\vspace{-1em}
\subsection{Review of Entity Abstraction Component}
	
In order to further consider the degree of matching between the key target information and the instruction information, we introduced entity abstract components to process the characteristic information under the instruction information. We adopt a similar approach to Suhr \cite{suhr2018learning}, replacing phrases in the sentences which refer to previously unseen entities with variables. E.g.,“Walk from kitchen to sofa” turns into “Walk from X1 to Y1”. Here X1 and Y1 are the feature points that match the perceived visual information. We use entity abstract components to extract different types of subjects (streets, restaurants, etc.) and number them in the order of occurrence of sentences \cite{paz2019run}. as the Figure \ref{image05} shows, the number is reset after each instruction is completed, so the model is still a small number of Vital Information is saved, so that the matching efficiency of the overall model is significantly improved \cite{iyer2017learning}. We use an encoder-decoder model with global attention, where the anonymized utterance is encoded using a bidirectional LSTM network.

\begin{equation}
	\centering
	{c}_{i} = \displaystyle\sum_{j=1}^k {\alpha}_{i, j} \cdot s_{j}
\end{equation}

\begin{equation}
	\centering
	{\alpha}_{i, j} = {\frac{exp({h_{i}^{T}Fs_{j}})}{\sum_{j=1}^{k}exp({h_{i}^{T}Fs_{j}})}}
\end{equation}

\begin{equation}
	\centering
	{h}_{i}, {m}_{i} = f({h}_{i-1}, {m}_{i-1}, {c}_{i-1})
\end{equation}

The attention weights ${\alpha}_{i, j}$ are computed using an inner product between the decoder hidden state for the current timestep ${h}_{i}$, and the hidden representation of the source token ${s}_{j}$. Where ${F}$ is a linear transformation. The decoder LSTM cell ${f}$ computes the next hidden state ${h}_{i}$, and cell state ${m}_{i}$ based on the previous hidden and cell states,${h}_{i-1}$,${m}_{i-1}$, the context vector of the previous timestep, ${c}_{i-1}$.

\begin{figure*}[h]
	\centering
	\includegraphics[scale=1]{image05.png}
	\caption{Examples of instruction for navigation. We use entity abstract components to extract different types of subjects (streets, restaurants, etc.) and number them in the order of occurrence of sentences.}
	\label{image05}
\end{figure*}
\vspace{-2em}
	
\subsection{Learning Detail}
	
The model is trained in a supervised manner. We follow the student-forcing approach to train our models. In each step, the monitoring signal with motion in the direction of the next landmark trains the motion prediction module. We use cross-entropy loss to train the action module and the matching module because they are planned as classification tasks. The total loss is the sum of the losses of all modules:
	
\begin{equation}
	\centering
	{Loss}_{all} = {Loss}_{vision-matching} + {Loss}_{track-matching} + {Loss}_{action}
\end{equation}

The loss of the two matching modules only takes effect at the landmarks of the landmarks, and these landmarks are more than the road nodes for calculating the motion loss and trajectory error loss. Therefore, we first train the matching networks separately for the matching task, and then integrate them with other components into overall training. We use ${Loss}_{all}$ to train the entire network.

\section{Experiments}
	
\subsection{Experimental Settings}
	
\textbf{Dataset.} We use the Room-to-Room (R2R) vision-and-language navigation dataset for our experimental evaluation\cite{anderson2018vision}. In this task, the agent starts at a certain location in an environment and is provided with a human-generated navigation instruction, that describes a path to a goal location. The agent needs to follow the instruction by taking multiple discrete actions (e.g. turning, moving) to navigate to the goal location, and executing a “stop” action to end the episode. Note that differently from some robotic navigation settings, here the agent is not provided with a goal image, but must identify from the textual description and environment whether it has reached the goal.
	- the Matterport3D navigation graphs, where each path consists of 5 to 7 discrete viewpoints and the average physical path length is 10m. Each path has three instructions written by humans, giving 21.5k instructions in total, with an average of 29 words per instruction. The dataset is split into training, validation, and test sets. The validation set is split into two parts: seen, where routes are sampled from environments seen during training, and unseen with environments that are not seen during training. All the test set routes belong to new environments unseen in the training and validation sets.

\textbf{Evaluation metrics.} Following previous work on the R2R task, our primary evaluation metrics are navigation error (NE), measuring the average distance between the end-location predicted by the follower agent and the true route’s end-location, and success rate (SR), the percentage of predicted end-locations within 3m of the true location. As in previous work, we also report the oracle success rate (OSR), measuring success rate at the closest point to the goal that the follower has visited along the route, allowing the agent to overshoot the goal without being penalized.

\textbf{Implementation details.} We produce visual feature vectors v using the output from the final convolutional layer of a ResNet trained on the ImageNet classification dataset. These visual features are fixed, and the ResNet is not updated during training. To better generalize to novel words in the vocabulary, we also experiment with using BERT to initialize the word-embedding vectors. We generate dynamic filters with 512 channels using a linear layer with dropout (p = 0.5).In our attention module, q and K have 128 channels and we apply a ReLU non-linearity after the linear transformation. For our action selection, we apply dropout with p = 0.5 to the policy hidden state before feeding it to the linear layer.
\vspace{-2em}
\subsection{Ablation Study}
\vspace{-2em}

\begin{table*}[h]
	\centering
	\caption{The ablation study of our architecture on the R2R validation group and our baseline model is the Speaker-Follower model. When the Vital Information matching module and the trajectory self-tuning module are added, the effect of the whole model is better.}
	{\begin{tabular}[c]{cccccccccc}
			\toprule[1pt]
			\multirow{2}{*}{{\textbf{Method}}} & \multirow{2}{*}{{\textbf{Num}}} & \multicolumn{1}{c}{{\textbf{Cross-model}}}  & \multicolumn{1}{c}{{\textbf{Track}} } &\multicolumn{4}{c}{{\textbf{Validation Seen}}} \\
			
			& &\textbf{Guide} &\textbf{Self-tuning} & \textbf{SR}$\uparrow$ & \textbf{NE}$\downarrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$\\
			\toprule[1pt]
			
			Speaker-Follower   & & & & 0.63 & 3.4 & 0.71 & - \\
			\toprule[1pt]
			\multirow{3}{*}{VIM-Net}  &1& $\surd$ &  & 0.68 &  3.31 &  0.76 & 0.59  \\
			   &2& & $\surd$ & 0.64 & \textbf{3.20} & 0.76 & 0.52  \\
				 &3& $\surd$ & $\surd$ & \textbf{0.69} &  3.23 & \textbf{0.78} & \textbf{0.64} \\
			\bottomrule[1pt]
		\end{tabular}
		\label{table_1}}
\end{table*}
\vspace{-2em}

\begin{table*}[h]
	\centering
	
	{\begin{tabular}[c]{ccccccccccccc}
			\toprule[1pt]
			\multirow{2}{*}{{\textbf{Model}}} & \multicolumn{4}{c}{{\textbf{Validation Unseen}}} & \multicolumn{4}{c}{{\textbf{Test Seen}}}\\
			
			& \textbf{SR}$\uparrow$ & \textbf{NE}$\downarrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$	& \textbf{SR}$\uparrow$ & \textbf{NE}$\downarrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$ & \\
			\toprule[1pt]
			
			Speaker-Follower   & 0.38 & 6.68 & 0.42 & - & 0.36 & 6.69 & 0.42 & 0.28\\
			\toprule[1pt]
			Cross-model Guide  & 0.41 & 5.94 & 0.53  & 0.35 & 0.43 &  5.97 &  0.51 & 0.34\\
			Track Self-tuning   & 0.42 & \textbf{5.57} & 0.45 & 0.34  & 0.41 & \textbf{5.59} & 0.43 & 0.33\\
			VIM-Net & \textbf{0.46} & 5.63 & \textbf{0.57} & \textbf{0.39} & \textbf{0.47} &  5.76 & \textbf{0.53} & \textbf{0.39}\\
			
			\bottomrule[1pt]
		\end{tabular}
		\label{table_2}}
\end{table*}

\begin{table*}[h]
	\centering
	{\begin{tabular}[c]{ccccccccccccc}
			\toprule[1pt]
			\multirow{2}{*}{{\textbf{Number of Epoch}}} & \multicolumn{3}{c}{{\textbf{Validation Unseen}}} & \multicolumn{3}{c}{{\textbf{Test Seen}}} & \multicolumn{3}{c}{{\textbf{Test Seen}}}\\
			
			& \textbf{SR}$\uparrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$	& \textbf{SR}$\uparrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$ & \textbf{SR}$\uparrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$ \\
			\toprule[1pt]
			0	& 0.13 & 0.15 & 0.12 & 0.11 & 0.13 & 0.10 & 0.11 & 0.13 & 0.11\\
			10   & 0.39 & 0.44 & 0.36 & 0.30 & 0.35 & 0.25  & 0.30 & 0.35 & 0.25\\
			20  & 0.60 & 0.65 & 0.58  & 0.39 & 0.48 &  0.33 & 0.39 & 0.45 &  0.32\\
			30   & 0.68 & 0.77 & 0.64 & 0.45 & 0.57 & 0.39 & 0.44 & 0.53 & 0.38 \\
			40 	& 0.69 & 0.78 & 0.64 & 0.45 & 0.57 & 0.39 & 0.44 & 0.53 & 0.39\\
			50	& 0.69 & 0.77 & 0.64 & 0.46 & 0.57 & 0.38 & 0.44 & 0.54 & 0.38\\
			60	& 0.68 & 0.78 & 0.65 & 0.46 & 0.56 & 0.37 & 0.45 & 0.54 & 0.38 \\

			\bottomrule[1pt]
		\end{tabular}
		\label{table_3}}
\end{table*}


The results presented in Table \ref{table_1} show, we test the impact of our implementation choices on VLN in our ablation study. First, we compare the VIM-Net model with a model that uses a simple replication data set to expand the data set to discuss the impact of the Vital Information matching module on the entire network. Then, we introduced the importance of using the trajectory self-tuning module to correct the entire navigation task.

\textbf{Cross-model guide module.}As the results show, the performance of the Vital Information matching model in data set processing largely exceeds the traditional data enhancement method of VLN. This is because the model can more accurately dig out the inner connection between the perceptual vision and text instructions under the input data set. Compared with the baseline model using purely replicated data sets, our Cross-model guide module improves the success rate by 5.

\textbf{Track Self-tuning module.}Our method is significantly better than the self-monitoring agent using greedy decoding. When the progress marker can use the features of each navigable direction previously accessed, but the trajectory self-tuning module is not available, the performance will not increase significantly (36 SR) . However, the use of the gated attention mechanism is good for extracting the overlap between the position of the landmark under the text instruction and the real trajectory, which means that the network can use this information to improve action selection. Compared with the baseline model that uses pure soft attention to compare the error of the entire trajectory, our method can achieve a moderate gain (41 SR), which reflects the purpose of intelligent navigation.

\begin{figure}[h]
	\centering
	\subfigure[Validation Seen]{
		\begin{minipage}[t]{0.33\linewidth}
			\centering
			\includegraphics[width=2in]{Figure_1.png}
		\end{minipage}%
	}%
	\subfigure[Validation Unseen]{
		\begin{minipage}[t]{0.33\linewidth}
			\centering
			\includegraphics[width=2in]{Figure_2.png}
		\end{minipage}%
	}%
	\subfigure[Test Unseen]{
		\begin{minipage}[t]{0.33\linewidth}
			\centering
			\includegraphics[width=2in]{Figure_3.png}
		\end{minipage}%
	}%
	\centering
	\caption{ Comparison of performance on the different condition: (a) Validation Seen, (b) Validation Unseen, and (c) Test Unseen.}
\end{figure}

\begin{figure}[h]
	\begin{minipage}{0.5\textwidth}
		\includegraphics[width=2.5in]{Figure_4.png}
	\end{minipage}
	\hfill
	\begin{minipage}{0.5\textwidth}
		\begin{table}[H]
			\vspace{-2em}
			\centering
			\caption{Comparison of the Test Unseen Set.}
			{\begin{tabular}[c]{ccccccccccccc}
					\toprule[1pt]
					\multirow{2}{*}{{\textbf{The Weight ${\lambda}$}}} & \multicolumn{4}{c}{{\textbf{Test Unseen}}} \\
					
					& \textbf{SR}$\uparrow$ & \textbf{OSR}$\uparrow$ & \textbf{SPL}$\uparrow$	&  \\
					\toprule[1pt]
					
					0.4   & 0.30 & 0.35 & 0.28 \\
					0.5  & 0.35 &  0.41 & 0.33  \\
					0.6    & 0.39 & 0.46 & 0.36  \\
					0.7 	& 0.43 & \textbf{0.54} & 0.38 \\
					0.75   & \textbf{0.47} &  0.53 & 0.39 \\
					0.8   & 0.44 & 0.48 & \textbf{0.41} \\
					0.9   & 0.34 & 0.39 & 0.35 \\
					\bottomrule[1pt]
				\end{tabular}
				\label{table_3}}
		\end{table}
	\end{minipage}
	\caption{Comparison of performance on the different weight of Scoring module. The best result is approximately obtained when the  is equal to 0.75.}
\end{figure}
We compared different results and plotted images to further observe the changes in the results of different training epoch. when ${{\lambda}}$ was equal to 0.75 in the formula ${{\lambda}{\Psi}_{t} + {\varphi}_{t}}$ in the VIM-net model, the effect of the entire model was optimal.

\section{Related Work}
In order to improve the accuracy of visual language navigation tasks, many scholars \cite{fried2018speaker,hwang2021joint,majumdar2020improving,ma2019regretful,wang2019reinforced,zhu2020vision} have made lots of contributions. The Speaker-Follower model is proposed in Fried. The model is mainly divided into two modules: Speaker module and Follower module. Speaker outputs the corresponding language label according to the path, and Follower is responsible for outputting the path according to the input text command, so that by duplicating the original data set The function of expanding the data set is achieved, and the action space of the environment has been changed. The original one can only rotate 90 degrees stiffly to any angle, which increases the freedom of the action space and makes it more accurate decision; Zhu made an integration of all the previous methods, first combining the three-alignment mechanism of seq2seq, reinforcement- learning and imitation learning, and adding the structure of the graph built in the entire scene, and optimizing the specific loss function to adapt to the new algorithm framework. However, the model proposed by the author still ignores the Vital Information of the entire navigation task, that is, typical landmark objects or obvious location words, and the error between the real trajectory and the text instruction, which leads to the effect of the entire model is still not ideal. Therefore, this paper proposes the VIM-Net model. In the network, we propose two main modules, the visual instruction matching module and the trajectory instruction matching module. The detailed information under the navigation task is deeply excavated, and this method fundamentally solves the above-mentioned problems.

\section{Conclusion}
Conventional navigation focuses on pre-constructing a map of the entire scene, and marking the initial location and destination location. The most suitable trajectory is derived by beam search or greedy algorithms, but the visual language navigation based on deep learning focuses on vision and text The navigation behavior is deduced, in which the visual and text processing are relatively independent, and each is processed by the more mature algorithms in the field, and then simply aligned and spliced, and the appropriate navigation behavior is judged under supervision. This kind of work undoubtedly saves labor costs and time costs, making it more generalizable. This research proposes a new type of deep neural network model VIM-Net as an effective tool to solve VLN tasks. The proposed model aims to use the past temporal context and the multi-modal background extracted with the joint multi-modal embedding module. In addition, VIM-Net, a new greedy local search algorithm with backtracking function, improves the task success rate and search efficiency. Finally, we verify the advantages of the proposed model through various experiments using the R2R benchmark dataset.
	
	%\begin{acknowledgements}
	
	%This research was funded by the National Natural Science Foundation of China under Grants 61872073, 61973093, 61901098, 61971118 and 61973063; the National Key Robot Project grant number$\backslash$2017YFB1300900; Shenyang NEU New Industrial Technology Research Institute (17-500-8-01). 
	
	%\end{acknowledgements}
	
\section*{Declarations}
\subsection*{Funding}
	
	%This research was funded by the National Natural Science Foundation of China under Grants 61872073, 61973093, 61901098, 61971118 and 61973063; the National Key Robot Project grant number$\backslash$2017YFB1300900; Shenyang NEU New Industrial Technology Research Institute (17-500-8-01). 
	
\subsection*{Conflicts of Interest/Competing Interests}
	
The authors declare that they have no conflicts of interest, and they have no competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
	
\subsection*{Availability of Data and Material}
	
All data generated or analysed during this study are included in this published article and its supplementary information files.
	
\subsection*{Code Availability}
	
Not applicable.
	
\subsection*{Authors' Contributions}
	
	%\textbf{Zixi Jia:} Methodology, Software, Writing - review and editing. \textbf{Lele Xue:} Conceptualization, Methodology, Investigation, Writing - original draft. \textbf{Jingyu Ru:} Validation, Formal analysis, Visualization, Software. \textbf{Zhou Wang:} Validation, Conceptualization, Visualization. \textbf{Minglin Dong:} Validation, Visualization, Editing. \textbf{Sikai Yang:} Software, Review, Editing. \textbf{Jiao Li:} Validation, Wring - review, Editing.
	
\subsection*{Ethics Approval}
	
Not applicable.
	
\subsection*{Consent to Participate}
	
Not applicable.
	
\subsection*{Consent for Publication}
	
Not applicable.
	
	% Authors must disclose all relationships or interests that 
	% could have direct or potential influence or impart bias on 
	% the work: 
	%
	% \section*{Conflict of interest}
	%
	% The authors declare that they have no conflict of interest.
	
	
	% BibTeX users please use one of
	%\bibliographystyle{spbasic}      % basic style, author-year citations
	%\bibliographystyle{spmpsci}      % mathematics and physical sciences
	%\bibliographystyle{spphys}       % APS-like style for physics
	%\bibliography{}   % name your BibTeX data base
	
	% Non-BibTeX users please use
	%\begin{thebibliography}
	%
	% and use \bibitem to create references. Consult the Instructions
	% for authors for reference list style.
	%
	%\reftitle{References}
	\bibliographystyle{ieeetr}
	\bibliography{coling}
	
	%\noindent \textbf{Zixi Jia} received the Ph.D. degree in pattern recognition and intelligent systems from Northeastern University, Shenyang, China, in 2009, where he is currently an Associate Professor and the Vice Dean of Faculty of Robot Science and Engineering. His research interests include artificial intelligence, robotics, big data and wireless sensor networks.\par
	
	%\noindent \textbf{Lele Xue} a graduate student of Faculty of Robot Science and Engineering, Northeastern University,Shenyang,China. His research intersts include computer vision and pattern recognition.\par
	
	%\noindent \textbf{Jingyu Ru}  received the  Ph.D. degrees in Northeastern University, Shenyang, China, in 2019. He is currently a post doctor as a staff in Robot Science and Engineering in Northeastern University, China. His research interests include artificial intelligence, big data, wireless sensor networks and robotics.\par
	
	%\noindent \textbf{Zhou Wang} a graduate student of Faculty of Robot Science and Engineering, Northeastern University,Shenyang,China. His research interst is computer vision.\par
	
	%\noindent \textbf{Minglin Dong} a graduate student of Faculty of Robot Science and Engineering, Northeastern University,Shenyang,China.The research intersts  include computer vision, natural language processing and multimodal fusion\par
	
	%\noindent \textbf{Sikai Yang} a graduate student of Faculty of Robot Science and Engineering, Northeastern University,Shenyang,China.The research interst is computer vision. \par
	
	%\noindent \textbf{Jiao Li} a graduate student of Faculty of Robot Science and Engineering, Northeastern University,Shenyang,China.The research interst  is  natural language processing. \par
	
	
	
\end{document}
% end of file template.tex

